{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f8b40f",
   "metadata": {},
   "source": [
    "# Anatomia de Uma Rede Neural | SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02a5c8",
   "metadata": {},
   "source": [
    "## 1. Introdu√ß√£o üìñ\n",
    "\n",
    "<p style='text-align: justify;'>Neste projeto, embarcamos em uma explora√ß√£o detalhada para desvendar os intricados detalhes da constru√ß√£o de modelos de Deep Learning. O objetivo principal √© fornecer um entendimento abrangente das etapas fundamentais envolvidas na constru√ß√£o desses modelos, utilizando as poderosas ferramentas PyTorch e PyTorch Lightning.</p>\n",
    "\n",
    "<p style='text-align: justify;'>O <a href=\"https://pytorch.org/get-started/pytorch-2.0/\" target=\"_blank\">PyTorch</a> √© uma biblioteca de aprendizado profundo de c√≥digo aberto que oferece a flexibilidade e a velocidade necess√°rias na pesquisa de aprendizado profundo. Ele suporta opera√ß√µes de tensor com acelera√ß√£o de GPU, fornece uma plataforma de aprendizado profundo que oferece versatilidade e velocidade, e se integra perfeitamente ao ecossistema Python.</p>\n",
    "\n",
    "<p style='text-align: justify;'>Por outro lado, o <a href=\"https://lightning.ai/docs/pytorch/stable/\" target=\"_blank\">PyTorch Lightning</a> √© uma estrutura leve que organiza o c√≥digo PyTorch. Ele permite que os pesquisadores se concentrem nas partes inovadoras de seus projetos, eliminando a necessidade de escrever c√≥digo repetitivo. Constru√≠do sobre o PyTorch, o PyTorch Lightning permite que voc√™ escale seus modelos sem a necessidade de reescrever seu c√≥digo.</p>\n",
    "\n",
    "\n",
    "<p style='text-align: justify;'>Este caderno vai al√©m de um simples tutorial; √© uma explora√ß√£o pr√°tica do mundo fascinante do Deep Learning. Este projeto √© fruto da minha forma√ß√£o como Engenheiro de Intelig√™ncia Artificial na <a href=\"https://www.datascienceacademy.com.br/start\" target=\"_blank\">Data Science Academy</a>. Com este caderno, esperamos n√£o apenas ensinar, mas tamb√©m inspirar voc√™ a explorar ainda mais as possibilidades do Deep Learning.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a591d8",
   "metadata": {},
   "source": [
    "## 2. Configura√ß√£o ‚öôÔ∏è\n",
    "\n",
    "### 2.1 Carga de Pacotes Python\n",
    "\n",
    "Este Jupyter Notebook utiliza v√°rias bibliotecas Python, cada uma com um prop√≥sito espec√≠fico:\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "1. **os**: `Interage com o sistema operacional, permitindo a manipula√ß√£o de arquivos e diret√≥rios.`\n",
    "2. **warnings**: `Emite mensagens de aviso ao usu√°rio.`\n",
    "4. **torch e lightning (pl)**: `PyTorch √© usado para aprendizado profundo e PyTorch Lightning organiza o c√≥digo PyTorch.`\n",
    "\n",
    "</div>\n",
    "\n",
    "<center><div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"imagens/np.png\" alt=\"numpy\" style=\"width:100px; margin: 15px;\"> \n",
    "    <img src=\"imagens/torch.jpeg\" alt=\"torch\" style=\"width:180px; margin: 15px;\"> \n",
    "    <img src=\"imagens/lightning.png\" alt=\"lightning\" style=\"width:190px; margin: 15px;\">\n",
    "</div></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c84d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente de desenvolvimento\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Lightning\n",
    "import lightning\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c8ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Igorando avisos desnecess√°rios\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f3383",
   "metadata": {},
   "source": [
    "### 2.2 Checando Vers√µes dos Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee64598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--= VERS√ïES DOS PACOTES UTILIZADOS =--\n",
      "  - Pytorch: 2.0.1+cu117\n",
      "  - Lightning: 2.0.9\n",
      "--= ------------------------------ =--\n"
     ]
    }
   ],
   "source": [
    "print(\"--= VERS√ïES DOS PACOTES UTILIZADOS =--\")\n",
    "print(f\"  - Pytorch: {torch.__version__}\")\n",
    "print(f\"  - Lightning: {lightning.__version__}\")\n",
    "print(\"--= ------------------------------ =--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a44666",
   "metadata": {},
   "source": [
    "### 2.3 Ambiente de Desenvolvimento e Reprodutibilidade dos Experimentos\n",
    "\n",
    "<p style='text-align: justify;'>A fun√ß√£o <strong>set_seed</strong> √© usada para definir a semente para geradores de n√∫meros aleat√≥rios no PyTorch. Isso √© √∫til para garantir que os experimentos sejam reproduz√≠veis, ou seja, que os mesmos resultados sejam obtidos sempre que o c√≥digo for executado com a mesma semente.</p>\n",
    "\n",
    "Aqui est√£o as funcionalidades de cada parte do c√≥digo:\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 10px;\">\n",
    "    \n",
    "1. `torch.manual_seed(seed)`: Define a semente para o gerador de n√∫meros aleat√≥rios do PyTorch para a CPU.\n",
    "\n",
    "2. `os.environ['PYTHONHASHSEED'] = str(seed)`: Define a semente para as fun√ß√µes hash do Python.\n",
    "\n",
    "3. `if torch.cuda.is_available()`: Verifica se uma GPU est√° dispon√≠vel.\n",
    "\n",
    "4. `torch.cuda.manual_seed_all(seed)`: Define a semente para todas as GPUs dispon√≠veis.\n",
    "\n",
    "5. `torch.backends.cudnn.deterministic = True`: Garante que o backend cuDNN use apenas algoritmos determin√≠sticos.\n",
    "\n",
    "6. `torch.backends.cudnn.benchmark = False`: Desativa o uso de um algoritmo de convolu√ß√£o heur√≠stico.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d21e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Define a semente para geradores de n√∫meros aleat√≥rios no PyTorch e no ambiente Python.\n",
    "    Isso garante que os resultados dos c√°lculos que usam n√∫meros aleat√≥rios sejam reproduz√≠veis.\n",
    "\n",
    "    Par√¢metros:\n",
    "        - seed (int): A semente para os geradores de n√∫meros aleat√≥rios.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CPU\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Chamando a fun√ß√£o set_seed()\n",
    "set_seed(seed=1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc6ab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo usado: cpu\n"
     ]
    }
   ],
   "source": [
    "# Dispositivo usado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo usado: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4acfba",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o e Carregamento dos Dados üíΩ\n",
    "\n",
    "### 3.1 Gerando Dados para o Problema XOR (OU Exclusivo)\n",
    "\n",
    "<center><img src=\"imagens/xor.png\" width=50%;></center>\n",
    "\n",
    "<p style='text-align: justify;'>Este c√≥digo est√° preparando um conjunto de dados para treinar um modelo de rede neural para resolver o problema XOR. O problema XOR √© um problema cl√°ssico em redes neurais que n√£o pode ser resolvido por uma √∫nica camada de perceptron, pois os dados do XOR n√£o s√£o linearmente separ√°veis.</p>\n",
    "\n",
    "Aqui est√° o que cada parte do c√≥digo faz:\n",
    "\n",
    "<div style=\"background-color: #FAF0E6; padding: 10px; border-radius: 10px;\">\n",
    "\n",
    "1. **Dados de entrada e sa√≠da**: As vari√°veis `dados_entrada` e `dados_saida` cont√™m os quatro poss√≠veis pares de entradas bin√°rias e suas respectivas sa√≠das para a opera√ß√£o XOR. Por exemplo, [0, 0] produz 0 e [0, 1] produz 1.\n",
    "\n",
    "2. **Dataset final**: A vari√°vel `dados_final` combina os dados de entrada e sa√≠da em uma √∫nica lista de tuplas. Cada tupla cont√©m um par de entrada e a sa√≠da correspondente.\n",
    "\n",
    "3. **DataLoader**: A vari√°vel `loader_treinamento` √© um DataLoader do PyTorch, que √© uma ferramenta para carregar os dados em lotes durante o treinamento de uma rede neural. Neste caso, o tamanho do lote √© definido como 1, o que significa que cada lote conter√° apenas um par de entrada-sa√≠da.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d70157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Creat_DataLoader():\n",
    "    \"\"\"\n",
    "    Cria um DataLoader com dados de entrada e sa√≠da para o problema XOR (OU Exclusivo).\n",
    "    \n",
    "    Os dados de entrada s√£o combina√ß√µes de 0s e 1s, e os dados de sa√≠da s√£o o resultado da opera√ß√£o XOR nos dados de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "    train_dataloader (DataLoader): Um DataLoader contendo os dados de entrada e sa√≠da.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dados de entrada\n",
    "    dados_entrada = [\n",
    "        Variable(torch.Tensor([0, 0])),\n",
    "        Variable(torch.Tensor([0, 1])),\n",
    "        Variable(torch.Tensor([1, 0])),\n",
    "        Variable(torch.Tensor([1, 1]))\n",
    "    ]\n",
    "    \n",
    "    # Dados de sa√≠da\n",
    "    dados_saida = [\n",
    "        Variable(torch.Tensor([0])),\n",
    "        Variable(torch.Tensor([1])),\n",
    "        Variable(torch.Tensor([1])),\n",
    "        Variable(torch.Tensor([0]))\n",
    "    ]\n",
    "    \n",
    "    # Compactando os dados\n",
    "    dados_compactados = list(zip(dados_entrada, dados_saida))\n",
    "    \n",
    "    # Criando DataLoader\n",
    "    train_dataloader = DataLoader(dataset=dados_compactados, batch_size=1)\n",
    "    \n",
    "    # Retornando os dados de treino\n",
    "    return train_dataloader\n",
    "\n",
    "# Chamando a fun√ß√£o Creat_DataLoader\n",
    "train_dataloader = Creat_DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f10740",
   "metadata": {},
   "source": [
    "## 4. SimpleNet: Uma Vis√£o Geral üß†\n",
    "\n",
    "A `SimpleNet` √© uma classe que implementa uma rede neural simples usando PyTorch Lightning. A rede consiste em uma camada de entrada, uma camada de sa√≠da e uma fun√ß√£o de ativa√ß√£o sigm√≥ide.\n",
    "\n",
    "No m√©todo `__init__`, a camada de entrada, a camada de sa√≠da, a fun√ß√£o de ativa√ß√£o sigm√≥ide e a fun√ß√£o de perda s√£o inicializadas. A camada de entrada √© uma camada linear que recebe 2 entradas e produz 4 sa√≠das. A camada de sa√≠da √© outra camada linear que recebe 4 entradas (do output da camada de entrada) e produz 1 sa√≠da. A fun√ß√£o de ativa√ß√£o sigm√≥ide √© usada para adicionar n√£o-linearidade ao modelo. A fun√ß√£o de perda usada √© a perda quadr√°tica m√©dia (MSE).\n",
    "\n",
    "O m√©todo `forward` realiza a passagem para frente na rede neural. A entrada passa pela camada de entrada, depois pela fun√ß√£o de ativa√ß√£o sigm√≥ide e finalmente pela camada de sa√≠da.\n",
    "\n",
    "O m√©todo `configure_optimizers` configura o otimizador para a rede neural. Ele usa o otimizador Adam com uma taxa de aprendizado de 0.01.\n",
    "\n",
    "O m√©todo `training_step` realiza uma etapa de treinamento na rede neural. A perda √© calculada comparando as sa√≠das da rede com as sa√≠das reais usando a fun√ß√£o de perda definida no construtor.\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src=\"imagens/simplenet.png\" width=40%;></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370889a",
   "metadata": {},
   "source": [
    "### 4.1 Fun√ß√£o de Custo (MSE)\n",
    "\n",
    "A fun√ß√£o de custo \\textit{Mean Squared Error} (MSE), ou Erro Quadr√°tico M√©dio em portugu√™s, √© uma das fun√ß√µes de perda mais utilizadas para problemas de regress√£o. Ela calcula a m√©dia dos quadrados das diferen√ßas entre os valores previstos e os valores reais.\n",
    "\n",
    "Aqui est√° a f√≥rmula matem√°tica para o MSE:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $n$ √© o n√∫mero total de exemplos no conjunto de dados\n",
    "- $y_i$ √© o valor real do i-√©simo exemplo\n",
    "- $\\hat{y}_i$ √© o valor previsto do i-√©simo exemplo\n",
    "\n",
    "\n",
    "O objetivo durante o treinamento de um modelo de aprendizado de m√°quina √© minimizar essa fun√ß√£o de perda. Isso significa que queremos que nossas previs√µes ($\\hat{y}_i$) estejam o mais pr√≥ximo poss√≠vel dos valores reais ($y_i$). Quanto menor o MSE, melhor nosso modelo √© capaz de realizar previs√µes precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4550e7f",
   "metadata": {},
   "source": [
    "### 4.2 Algoritmo Adam (Adaptive Moment Estimation)\n",
    "\n",
    "O algoritmo Adam (Adaptive Moment Estimation) √© um m√©todo de otimiza√ß√£o que pode ser usado em vez dos procedimentos cl√°ssicos de descida de gradiente estoc√°stico para atualizar os pesos da rede de forma iterativa com base nos dados de treinamento.\n",
    "\n",
    "Adam √© uma combina√ß√£o dos m√©todos AdaGrad e RMSProp, que s√£o outros algoritmos de otimiza√ß√£o. Ele calcula taxas de aprendizado adaptativas para diferentes par√¢metros. Em outras palavras, ele computa m√©dias m√≥veis tanto do gradiente quanto do quadrado do gradiente, e essas m√©dias s√£o usadas para dimensionar a taxa de aprendizado.\n",
    "\n",
    "Aqui est√£o as f√≥rmulas matem√°ticas para o Adam:\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $m_t$ e $v_t$ s√£o estimativas do primeiro momento (a m√©dia) e do segundo momento (a vari√¢ncia n√£o centralizada) do gradiente, respectivamente.\n",
    "- $\\beta_1$ e $\\beta_2$ s√£o os fatores de decaimento para essas estimativas.\n",
    "- $g_t$ √© o gradiente no tempo $t$.\n",
    "- $\\hat{m}_t$ e $\\hat{v}_t$ s√£o vers√µes corrigidas por vi√©s de $m_t$ e $v_t$.\n",
    "- $\\alpha$ √© a taxa de aprendizado.\n",
    "- $\\epsilon$ √© um termo de suaviza√ß√£o para evitar a divis√£o por zero.\n",
    "- $\\theta_t$ √© o par√¢metro atualizado no tempo $t$.\n",
    "\n",
    "\n",
    "O algoritmo Adam √© bastante eficaz e requer pouca configura√ß√£o de mem√≥ria, sendo uma escolha popular para redes neurais profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7df75",
   "metadata": {},
   "source": [
    "### 4.3 Retropropaga√ß√£o no SimpleNet\n",
    "\n",
    "O processo de retropropaga√ß√£o (Backward Propagation) √© um algoritmo usado em redes neurais para calcular o gradiente da fun√ß√£o de perda em rela√ß√£o aos pesos da rede. Ele √© chamado de \"backpropagation\" porque o c√°lculo do gradiente √© feito de tr√°s para frente, come√ßando da fun√ß√£o de perda e indo at√© as camadas de entrada.\n",
    "\n",
    "Aqui est√° uma descri√ß√£o detalhada do processo de retropropaga√ß√£o na sua rede SimpleNet:\n",
    "\n",
    "1. **C√°lculo do Erro**: Primeiro, calculamos o erro da previs√£o usando a fun√ß√£o de custo MSE. Para um √∫nico exemplo, o erro √© dado por:\n",
    "\n",
    "    $$E = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "    onde $y$ √© o valor real e $\\hat{y}$ √© o valor previsto pela rede.\n",
    "\n",
    "2. **Gradiente na Camada de Sa√≠da**: O pr√≥ximo passo √© calcular o gradiente do erro em rela√ß√£o aos pesos da camada de sa√≠da. Usando a regra da cadeia, temos:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial w_{out}} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w_{out}}$$\n",
    "\n",
    "    Onde $w_{out}$ s√£o os pesos da camada de sa√≠da. Calculamos cada parte separadamente:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial \\hat{y}} = -(y - \\hat{y})$$\n",
    "\n",
    "    $$\\frac{\\partial \\hat{y}}{\\partial w_{out}} = \\hat{y}(1 - \\hat{y}) \\cdot h_{out}$$\n",
    "\n",
    "    Onde $h_{out}$ √© a sa√≠da da camada oculta. Portanto, o gradiente na camada de sa√≠da √©:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial w_{out}} = -(y - \\hat{y}) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot h_{out}$$\n",
    "\n",
    "3. **Gradiente na Camada de Entrada**: Da mesma forma, podemos calcular o gradiente do erro em rela√ß√£o aos pesos da camada de entrada:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial w_{in}} = \\frac{\\partial E}{\\partial h_{out}} \\cdot \\frac{\\partial h_{out}}{\\partial w_{in}}$$\n",
    "\n",
    "    Onde $w_{in}$ s√£o os pesos da camada de entrada. Novamente, calculamos cada parte separadamente:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial h_{out}} = \\frac{\\partial E}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h_{out}} = -(y - \\hat{y}) \\cdot \\hat{y}(1 - \\hat{y})$$\n",
    "\n",
    "    $$\\frac{\\partial h_{out}}{\\partial w_{in}} = h_{out}(1 - h_{out}) \\cdot x$$\n",
    "\n",
    "    Onde $x$ √© a entrada para a rede. Portanto, o gradiente na camada de entrada √©:\n",
    "\n",
    "    $$\\frac{\\partial E}{\\partial w_{in}} = -(y - \\hat{y}) \\cdot \\hat{y}(1 - \\hat{y}) \\cdot h_{out}(1 - h_{out}) \\cdot x$$\n",
    "\n",
    "4. **Atualiza√ß√£o dos Pesos**: Finalmente, usamos o algoritmo Adam para atualizar os pesos em ambas as camadas. O Adam ajusta a taxa de aprendizado para cada peso individualmente, com base nas estimativas do primeiro e segundo momentos do gradiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c00ad1",
   "metadata": {},
   "source": [
    "## 5. Implementa√ß√£o da SimpleNet üèóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9dac878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(lightning.LightningModule):\n",
    "    \n",
    "    # M√©todo construtor\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.input_layer  = nn.Linear(2, 4)\n",
    "        self.output_layer = nn.Linear(4, 1)\n",
    "        self.sigmoid      = nn.Sigmoid()\n",
    "        self.loss         = nn.MSELoss()\n",
    "        \n",
    "    # M√©todo da passada para frente (forward)\n",
    "    def forward(self, input):\n",
    "        x = self.input_layer(input)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    # M√©todo de otimiza√ß√£o\n",
    "    def configure_optimizers(self):\n",
    "        params    = self.parameters()\n",
    "        optimizer = optim.Adam(params=params, lr=0.01)\n",
    "        return optimizer\n",
    "    \n",
    "    # M√©todo das passadas de treinamento\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y    = batch\n",
    "        outputs = self(x)\n",
    "        loss    = self.loss(outputs, y)\n",
    "        return loss\n",
    "    \n",
    "# Instanciando a classe SimpleNet\n",
    "modelo = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656f996",
   "metadata": {},
   "source": [
    "## 6. Treinamento da SimpleNet üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce638d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
